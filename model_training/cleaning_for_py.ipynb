{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dropout, Flatten, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import tensorflow.keras.utils\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.replace({'neutral':0, 'negative':-1, 'positive':1}, inplace=True)\n",
    "y = tensorflow.keras.utils.to_categorical(y, num_classes=3, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing SpaCy-transformer pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']\n"
     ]
    }
   ],
   "source": [
    "name = \"bert-base-uncased\"\n",
    "nlp = TransformersLanguage(trf_name=name, meta={\"lang\": \"en\"})\n",
    "nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))\n",
    "nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_data):\n",
    "    clean_X = []\n",
    "    for text in tqdm.tqdm(text_data):\n",
    "        doc = nlp(text)\n",
    "        word_id = doc._.trf_word_pieces\n",
    "        clean_X.append(word_id)\n",
    "    max_lenght = len(max(clean_X, key=len))    \n",
    "    word_vec_X = sequence.pad_sequences(clean_X, maxlen = max_lenght, padding='pre')\n",
    "    pd.DataFrame(word_vec_X).to_csv(\"word_vec_X.csv\", index=None)\n",
    "    return word_vec_X\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2020,  2183,   102],\n",
       "       [    0,     0,     0, ...,   999,   999,   102],\n",
       "       [    0,     0,     0, ...,  1012,  1012,   102],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 22038, 20348,   102],\n",
       "       [    0,     0,     0, ...,  1008,  1012,   102],\n",
       "       [    0,     0,     0, ...,  1007,  1007,   102]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_X = pd.read_csv('word_vec_X.csv')\n",
    "word_vec_X = word_vec_X.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./uncased_L-12_H-768_A-12\"\n",
    "bert_params = bert.params_from_pretrained_ckpt(model_dir)\n",
    "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 112\n",
    "l_input_ids = tensorflow.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "\n",
    "output = l_bert(l_input_ids)\n",
    "\n",
    "cls_out = tensorflow.keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "cls_out = Dropout(0.5)(cls_out)\n",
    "\n",
    "logits = Dense(768, activation=\"tanh\")(cls_out)\n",
    "logits = Dropout(0.5)(logits)\n",
    "\n",
    "logits = Dense(units=3,activation=\"softmax\")(logits)\n",
    "\n",
    "model = tensorflow.keras.Model(inputs=l_input_ids, outputs=logits)\n",
    "model.build(input_shape=(None, max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 196 BERT weights from: ./uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f501408aeb8> (prefix:bert_2). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ckpt_file   = os.path.join(model_dir, \"bert_model.ckpt\")\n",
    "bert.load_stock_weights(l_bert, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16488 samples, validate on 4122 samples\n",
      "Epoch 1/2\n",
      "16488/16488 [==============================] - 868s 53ms/sample - loss: 0.4845 - accuracy: 0.7655 - val_loss: 0.3289 - val_accuracy: 0.8560\n",
      "Epoch 2/2\n",
      "16488/16488 [==============================] - 827s 50ms/sample - loss: 0.3247 - accuracy: 0.8620 - val_loss: 0.3504 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b3e43f128>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=2, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85      8335\n",
      "           1       0.91      0.87      0.89      6444\n",
      "           2       0.89      0.85      0.87      5831\n",
      "\n",
      "    accuracy                           0.86     20610\n",
      "   macro avg       0.87      0.86      0.87     20610\n",
      "weighted avg       0.87      0.86      0.86     20610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Xtrain, batch_size=64, verbose=0)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "ytrain_bool = np.argmax(ytrain, axis=1)\n",
    "print(classification_report(ytrain_bool, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76      2782\n",
      "           1       0.84      0.81      0.83      2138\n",
      "           2       0.81      0.77      0.79      1950\n",
      "\n",
      "    accuracy                           0.79      6870\n",
      "   macro avg       0.80      0.79      0.79      6870\n",
      "weighted avg       0.79      0.79      0.79      6870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(Xtest, batch_size=64, verbose=0)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "ytest_bool = np.argmax(ytest, axis=1)\n",
    "print(classification_report(ytest_bool, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json, custom_objects={\"BertModelLayer\": bert.BertModelLayer})\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
